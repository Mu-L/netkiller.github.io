<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>10.3. Tensor 张量</title><link rel="stylesheet" type="text/css" href="../docbook.css" /><meta name="generator" content="DocBook XSL Stylesheets Vsnapshot" /><meta name="keywords" content="php,pear,pecl,phar, python, , " /><link rel="home" href="../index.html" title="Netkiller Python 手札" /><link rel="up" href="index.html" title="第 10 章 PyTorch" /><link rel="prev" href="ch10s02.html" title="10.2. 显卡" /><link rel="next" href="pytorch.dataset.html" title="10.4. Dataset" /></head><body><a xmlns="" href="//www.netkiller.cn/">Home</a> | <a xmlns="" href="//netkiller.github.io/">简体中文</a> | <a xmlns="" href="http://netkiller.sourceforge.net/">繁体中文</a> | <a xmlns="" href="/journal/index.html">杂文</a>
		| <a xmlns="" href="https://github.com/netkiller">Github</a> | <a xmlns="" href="https://zhuanlan.zhihu.com/netkiller">知乎专栏</a> | <a xmlns="" href="https://www.facebook.com/bg7nyt">Facebook</a> | <a xmlns="" href="http://cn.linkedin.com/in/netkiller/">Linkedin</a> | <a xmlns="" href="https://www.youtube.com/user/bg7nyt/videos">Youtube</a> | <a xmlns="" href="//www.netkiller.cn/home/donations.html">打赏(Donations)</a> | <a xmlns="" href="//www.netkiller.cn/home/about.html">About</a><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">10.3. Tensor 张量</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="ch10s02.html">上一页</a> </td><th width="60%" align="center">第 10 章 PyTorch</th><td width="20%" align="right"> <a accesskey="n" href="pytorch.dataset.html">下一页</a></td></tr></table><hr /></div><table xmlns=""><tr><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=watch&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=fork&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;type=follow&amp;count=true&amp;size=large" height="30" width="240" frameborder="0" scrolling="0" style="width:240px; height: 30px;" allowTransparency="true"></iframe></td><td></td><td><a href="https://zhuanlan.zhihu.com/netkiller"><img src="/images/logo/zhihu-card-default.svg" height="25" /></a></td><td valign="middle"><a href="https://zhuanlan.zhihu.com/netkiller">知乎专栏</a></td><td></td><td></td><td></td><td></td></tr></table><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="pytorch.tensor"></a>10.3. Tensor 张量</h2></div></div></div>
		
		<div class="literallayout"><p><br />
		<br />
Scalar(标量)：指一个数值<br />
Vector(向量)：指一维数组<br />
Matrix(矩阵)：指二维数组<br />
Tensor(张量)：指大于二维的数组，即多位数组<br />
		<br />
		</p></div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1021"></a>10.3.1. 创建静态 Tensor</h3></div></div></div>
			
			<pre class="programlisting">
			
import torch

tensor = torch.tensor([5, 8, 3, 7, 2, 4, 1, 0, 6, 9])
print(tensor)			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1022"></a>10.3.2. Tensor 尺寸</h3></div></div></div>
			
			<pre class="programlisting">
			
import torch

tensor = torch.eye(2, 3)
print(tensor)
print(tensor.size())
print(tensor.size(0))
print(tensor.size(1))			
				
			</pre>
			<pre class="screen">
			
tensor([[1., 0., 0.],
        [0., 1., 0.]])
torch.Size([2, 3])
2
3			
			
			</pre>
		</div>
		
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1023"></a>10.3.3. 创建连续数列的 Tensor</h3></div></div></div>
			
			<pre class="programlisting">
			
import torch

tensor = torch.range(1, 10)

print(tensor)			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1024"></a>10.3.4. 创建0数据的 Tensor</h3></div></div></div>
			
			<p>创建一个 3x3 的 Tensor</p>
			<pre class="programlisting">
			
import torch

x = torch.zeros(3, 3)
print(x)			
			
			</pre>
			<p>输出结果</p>
			<pre class="screen">
			
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1025"></a>10.3.5. 判断变量是否为 Tensor</h3></div></div></div>
			
			<pre class="programlisting">
			
import torch

x = [1, 2, 3, 4, 5]
print(torch.is_tensor(x))

x1 = torch.rand(1, 2)
print(torch.is_tensor(x1))			
			
			</pre>
			<p>输出结果</p>
			<pre class="screen">
			
False
True			
			
			</pre>

		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1026"></a>10.3.6. 统计 Tensor 中的元素数量</h3></div></div></div>
			
			<pre class="programlisting">
			
import torch

x = torch.zeros(3, 3)
print(x)
print(torch.numel(x))	
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1027"></a>10.3.7. 创建对角线为1的 Tensor</h3></div></div></div>
			
			<pre class="programlisting">
			
import torch

x = torch.eye(3, 3)
print(x)
print(torch.numel(x))			
			
			</pre>
			<p>输出结果</p>
			<pre class="screen">
			
tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]])
9			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1028"></a>10.3.8. 将 numpy 转换成 tensor</h3></div></div></div>
			
			<pre class="programlisting">
			
import numpy
import torch

array = numpy.array([1, 2, 3, 4, 5])
print(array)
tensor = torch.from_numpy(array);
print(tensor)			
			
			</pre>
			<p>输出结果</p>
			<pre class="programlisting">
			
[1 2 3 4 5]
tensor([1, 2, 3, 4, 5])
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1029"></a>10.3.9. 切分</h3></div></div></div>
			
			<pre class="programlisting">
			
tensor = torch.linspace(1, 9, 9)
print(tensor)
			
			</pre>
			<pre class="screen">
			
tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1030"></a>10.3.10. 均匀分布数列</h3></div></div></div>
			
			<p>返回值在0～1之间</p>
			<pre class="programlisting">
			
tensor = torch.rand(10)
print(tensor)	
			
			</pre>
			<pre class="screen">
			
tensor([0.8865, 0.3672, 0.7740, 0.6729, 0.8264, 0.0811, 0.4243, 0.7992, 0.6984,
        0.1110])
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1031"></a>10.3.11. 正态分布数列</h3></div></div></div>
			
			<p>返回均值0，方差为1</p>
			<pre class="programlisting">
			
tensor = torch.randn(10)
print(tensor)			
			
			</pre>
			<p></p>
			<pre class="screen">
			
tensor([-0.3631,  0.9288,  0.5677,  0.5674,  0.2578, -0.4731, -0.3581, -0.6288,
        -1.1815, -0.1885])			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1032"></a>10.3.12. 随机数列</h3></div></div></div>
			
			<pre class="programlisting">
			
import torch

tensor = torch.randperm(10)
			
			</pre>
			<p>输出结果</p>
			<pre class="screen">
			
tensor([5, 8, 3, 6, 0, 7, 1, 4, 2, 9])
			
			</pre>
			<pre class="programlisting">
			
import torch

tensor = torch.randint(1, 10, (3, 3))
print(tensor)			
			
			</pre>
			<p>输出结果</p>
			<pre class="screen">
			
tensor([[6, 1, 9],
        [4, 3, 6],
        [4, 8, 4]])
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="torch.arange"></a>10.3.13. arange 创建等差数列</h3></div></div></div>
			
			<p>arange(start=0, end, step=1, *, out=None, dtype=None,
				layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor
			</p>
			<p>参数</p>
			<div class="literallayout"><p><br />
		<br />
start (数字) – 起始值。默认值：0。<br />
end (数字) – 结束值<br />
step (数字) – 每对相邻数字之间的步长。默认值：1。<br />
		<br />
			</p></div>
			<p>关键字参数</p>
			<div class="literallayout"><p><br />
		<br />
out (张量, 可选) – 输出张量。<br />
dtype (torch.dtype，可选) – 据类型。默认值：如果为 None，则使用全局默认值（请参阅 torch.set_default_dtype()）。如果未提供 dtype，则从其他输入参数推断数据类型。如果 start、end 或 stop 中的任何一个是浮点数，则 dtype 推断为默认 dtype，请参阅 get_default_dtype()。否则，dtype 推断为 torch.int64。<br />
layout (torch.layout，可选) – 布局。默认值：torch.strided。<br />
device (torch.device，可选) – 目标设备。默认值：如果为None，则使用默认张量类型当前的设备（参见 torch.set_default_device()）。对于 CPU 张量类型，device 将为 CPU；对于 CUDA 张量类型，device 将为当前的 CUDA 设备。<br />
requires_grad (bool, 可选) – autograd 是否应记录返回张量上的操作。默认值：False。				<br />
		<br />
			</p></div>
			<pre class="programlisting">
		
&gt;&gt;&gt; torch.arange(5)  # 默认以 0 为起点
tensor([ 0,  1,  2,  3,  4])
&gt;&gt;&gt; torch.arange(1, 4)  # 默认间隔为 1
tensor([ 1,  2,  3])
&gt;&gt;&gt; torch.arange(1, 2.5, 0.5)  # 指定间隔 0.5
tensor([ 1.0000,  1.5000,  2.0000])		
		
			</pre>
			<p></p>
			<pre class="programlisting">
		
import torch
from torch.utils.data import Dataset, DataLoader
#
data = torch.arange(15)
print(data)		
		
			</pre>
			<pre class="screen">
		
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])		
		
			</pre>
			<pre class="programlisting">
			
import torch

tensor = torch.arange(1, 10, 1)
print(tensor)
tensor = torch.arange(1, 10, 2)
print(tensor)
tensor = torch.arange(1, 10, 3)
print(tensor)
			
			</pre>
			<p>输出结果</p>
			<pre class="screen">
			
tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
tensor([1, 3, 5, 7, 9])
tensor([1, 4, 7])			
			
			</pre>
			<pre class="programlisting">
		
data = torch.arange(15).reshape(5,3)
print(data)		
		
			</pre>
			<pre class="screen">
		
tensor([[ 0,  1,  2],
        [ 3,  4,  5],
        [ 6,  7,  8],
        [ 9, 10, 11],
        [12, 13, 14]])		
		
			</pre>
		</div>

		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1033"></a>10.3.14. 获取最小值和最大值的索引</h3></div></div></div>
			
			<pre class="programlisting">
			
import torch

tensor = torch.randint(1, 10, (3, 3))
print(tensor)
min = torch.argmin(tensor, dim=0)
print(min)
max = torch.argmax(tensor, dim=0)
print(max)			
			
			</pre>
			<p>输出结果</p>
			<pre class="screen">
			
tensor([[5, 1, 4],
        [7, 7, 8],
        [2, 2, 9]])
tensor([2, 0, 0])
tensor([1, 1, 2])			
			
			</pre>
			<p>最小值返回索引 tensor([2, 0, 0])，2
				表示列第三个，0表示第二列第一个，最后一个0表示第三列第一个，最终可以获得 2，1，4 三个数据
			</p>
			<p>最大值返回索引 tensor([1, 1, 2])，对应数据 7，7，9</p>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1034"></a>10.3.15. 连接两个 Tensor</h3></div></div></div>
			
			<p>垂直连续，追加数据</p>
			<pre class="programlisting">
			
import torch

tensor1 = torch.randint(1, 10, (3, 3))
print(tensor1)

tensor2 = torch.randint(1, 10, (3, 3))
print(tensor2)

tensor = torch.cat((tensor1, tensor2))
print(tensor)			
			
			</pre>
			<p>输出结果</p>
			<pre class="screen">
			
tensor([[1, 7, 5],
        [8, 6, 8],
        [6, 9, 2]])
tensor([[6, 5, 6],
        [4, 2, 2],
        [5, 9, 3]])
tensor([[1, 7, 5],
        [8, 6, 8],
        [6, 9, 2],
        [6, 5, 6],
        [4, 2, 2],
        [5, 9, 3]])			
			
			</pre>
			<p>水平连接，扩展数据</p>
			<pre class="programlisting">
			
import torch

tensor1 = torch.randint(1, 10, (3, 3))
print(tensor1)

tensor2 = torch.randint(1, 10, (3, 3))
print(tensor2)

tensor = torch.cat((tensor1, tensor2), dim=1)
print(tensor)			
			
			</pre>
			<p>输出结果</p>
			<pre class="screen">
			
tensor([[6, 1, 6],
        [9, 4, 9],
        [5, 7, 5]])
tensor([[5, 9, 9],
        [2, 3, 8],
        [5, 2, 8]])
tensor([[6, 1, 6, 5, 9, 9],
        [9, 4, 9, 2, 3, 8],
        [5, 7, 5, 5, 2, 8]])
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1035"></a>10.3.16. 数据切块</h3></div></div></div>
			
			<pre class="programlisting">
			
import torch

tensor = torch.randint(1, 10, (3, 3))
print(tensor)

# 将 tensor 切成 3 份
tensor1 = torch.chunk(tensor, 3)
print(tensor1)

# 基于 Y 轴将 tensor 切成 3 份
tensor2 = torch.chunk(tensor, 3, dim=1)
print(tensor2)			
			
			</pre>
			<p>输出结果</p>
			<pre class="screen">
			
tensor([[7, 5, 6],
        [3, 5, 5],
        [2, 1, 8]])
(tensor([[7, 5, 6]]), tensor([[3, 5, 5]]), tensor([[2, 1, 8]]))
(tensor([[7],
        [3],
        [2]]), tensor([[5],
        [5],
        [1]]), tensor([[6],
        [5],
        [8]]))			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1036"></a>10.3.17. 通过索引下标选择数据</h3></div></div></div>
			
			<pre class="programlisting">
			
import torch

x = torch.randn(6, 6)
print(x)
index = torch.tensor([1, 3, 5])

tensor = torch.index_select(input=x, dim=0, index=index)
print(tensor)			
			
			</pre>
			<p>输出结果</p>
			<pre class="screen">
			
tensor([[ 0.2502,  0.0171,  0.0468, -1.1985,  0.1249, -0.2472],
        [-1.0763, -0.9561,  0.7500,  0.0903, -0.2404,  0.7940],
        [-0.2090, -1.1881,  0.6415, -1.8224, -1.2121,  0.0428],
        [-1.3796, -0.3021, -0.0945,  0.1808, -0.1802, -1.6069],
        [-1.0586,  0.0227, -0.4314, -0.4522, -0.6734,  0.0220],
        [-0.3171, -0.9069,  1.9850,  0.9845, -0.5600, -1.3951]])
tensor([[-1.0763, -0.9561,  0.7500,  0.0903, -0.2404,  0.7940],
        [-1.3796, -0.3021, -0.0945,  0.1808, -0.1802, -1.6069],
        [-0.3171, -0.9069,  1.9850,  0.9845, -0.5600, -1.3951]])			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1037"></a>10.3.18. 分割</h3></div></div></div>
			
			<pre class="programlisting">
			
import torch

tensor = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
print(tensor)
print(torch.split(tensor, 3))			
			
			</pre>
			<pre class="screen">
			
tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]), tensor([10]))			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1038"></a>10.3.19. 矩阵转换</h3></div></div></div>
			
			<p>行列转换</p>
			<pre class="programlisting">
			
import torch

tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
print(tensor)
print(tensor.t())
print(tensor.transpose(1, 0))
			
			</pre>
			<p>输出结果</p>
			<pre class="screen">
			
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
tensor([[1, 4, 7],
        [2, 5, 8],
        [3, 6, 9]])	
tensor([[1, 4, 7],
        [2, 5, 8],
        [3, 6, 9]])	        
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1039"></a>10.3.20. 矩阵运算</h3></div></div></div>
			
			<pre class="programlisting">
			
import torch

tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
print(tensor)
print(torch.add(tensor, 1))
print(torch.mul(tensor, 2))			
			
			</pre>
			<pre class="programlisting">
			
import torch 
a=torch.tensor([[1,2,3],[4,5,6]])
b=torch.tensor([[2,3,4],[5,6,7]])
c=torch.mul(a,b)
print('a:',a)
print('b:',b)
print('c:',c)			
			
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id1040"></a>10.3.21. view</h3></div></div></div>
			
			<pre class="programlisting">
			
import torch

tensor = torch.arange(9)
print(tensor)
print(tensor.size())
tmp = tensor.view(3, 3)
print(tmp)
print(tmp.size())			
			
			</pre>
			<p>tensor 转为 3x3 矩阵</p>
			<pre class="screen">
			
tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])
torch.Size([9])
tensor([[0, 1, 2],
        [3, 4, 5],
        [6, 7, 8]])
torch.Size([3, 3])			
			
			</pre>
			<pre class="programlisting">
			
			
			</pre>
		</div>
	</div><script xmlns="" type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=r5HG&amp;d=9mi5r_kkDC8uxG8HuY3p4-2qgeeVypAK9vMD-2P6BYM"></script><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ch10s02.html">上一页</a> </td><td width="20%" align="center"><a accesskey="u" href="index.html">上一级</a></td><td width="40%" align="right"> <a accesskey="n" href="pytorch.dataset.html">下一页</a></td></tr><tr><td width="40%" align="left" valign="top">10.2. 显卡 </td><td width="20%" align="center"><a accesskey="h" href="../index.html">起始页</a></td><td width="40%" align="right" valign="top"> 10.4. Dataset</td></tr></table></div><script xmlns="">
			(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

			ga('create', 'UA-11694057-1', 'auto');
			ga('send', 'pageview');

		</script><script xmlns="" async="async">
			var _hmt = _hmt || [];
			(function() {
			var hm = document.createElement("script");
			hm.src = "https://hm.baidu.com/hm.js?93967759a51cda79e49bf4e34d0b0f2c";
			var s = document.getElementsByTagName("script")[0];
			s.parentNode.insertBefore(hm, s);
			})();
</script><script xmlns="" async="async">
			(function(){
			var bp = document.createElement('script');
			var curProtocol = window.location.protocol.split(':')[0];
			if (curProtocol === 'https') {
			bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
			}
			else {
			bp.src = 'http://push.zhanzhang.baidu.com/push.js';
			}
			var s = document.getElementsByTagName("script")[0];
			s.parentNode.insertBefore(bp, s);
			})();
</script></body></html>
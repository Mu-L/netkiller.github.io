<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>第 20 章 Crawler</title><link rel="stylesheet" type="text/css" href="../docbook.css" /><meta name="generator" content="DocBook XSL Stylesheets Vsnapshot" /><meta name="keywords" content="php,pear,pecl,phar, python, , " /><link rel="home" href="../index.html" title="Netkiller Python 手札" /><link rel="up" href="../pt04.html" title="部分 IV. Python 数据分析" /><link rel="prev" href="../pt04.html" title="部分 IV. Python 数据分析" /><link rel="next" href="../pandas/index.html" title="第 21 章 Pandas - Python Data Analysis Library" /></head><body><a xmlns="" href="//www.netkiller.cn/">Home</a> | <a xmlns="" href="//netkiller.github.io/">简体中文</a> | <a xmlns="" href="http://netkiller.sourceforge.net/">繁体中文</a> | <a xmlns="" href="/journal/index.html">杂文</a>
		| <a xmlns="" href="https://github.com/netkiller">Github</a> | <a xmlns="" href="https://zhuanlan.zhihu.com/netkiller">知乎专栏</a> | <a xmlns="" href="https://www.facebook.com/bg7nyt">Facebook</a> | <a xmlns="" href="http://cn.linkedin.com/in/netkiller/">Linkedin</a> | <a xmlns="" href="https://www.youtube.com/user/bg7nyt/videos">Youtube</a> | <a xmlns="" href="//www.netkiller.cn/home/donations.html">打赏(Donations)</a> | <a xmlns="" href="//www.netkiller.cn/home/about.html">About</a><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">第 20 章 Crawler</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="../pt04.html">上一页</a> </td><th width="60%" align="center">部分 IV. Python 数据分析</th><td width="20%" align="right"> <a accesskey="n" href="../pandas/index.html">下一页</a></td></tr></table><hr /></div><table xmlns=""><tr><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=watch&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=fork&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;type=follow&amp;count=true&amp;size=large" height="30" width="240" frameborder="0" scrolling="0" style="width:240px; height: 30px;" allowTransparency="true"></iframe></td><td></td><td><a href="https://zhuanlan.zhihu.com/netkiller"><img src="/images/logo/zhihu-card-default.svg" height="25" /></a></td><td valign="middle"><a href="https://zhuanlan.zhihu.com/netkiller">知乎专栏</a></td><td></td><td></td><td></td><td></td></tr></table><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a id="index"></a>第 20 章 Crawler</h2></div></div></div><div class="toc"><p><strong>目录</strong></p><dl class="toc"><dt><span class="section"><a href="index.html#scrapy">20.1. Scrapy - Python web scraping and crawling framework</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#setup">20.1.1. 安装 scrapy 开发环境</a></span></dt><dt><span class="section"><a href="index.html#scrapy">20.1.2. scrapy 命令</a></span></dt><dt><span class="section"><a href="index.html#shell">20.1.3. Scrapy Shell</a></span></dt><dt><span class="section"><a href="index.html#tutorial">20.1.4. 爬虫项目</a></span></dt><dt><span class="section"><a href="index.html#images">20.1.5. 下载图片</a></span></dt><dt><span class="section"><a href="index.html#xpath">20.1.6. xpath</a></span></dt></dl></dd></dl></div>
	
	<div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="scrapy"></a>20.1. Scrapy - Python web scraping and crawling framework</h2></div></div></div>
		
		<p>https://scrapy.org</p>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="setup"></a>20.1.1. 安装 scrapy 开发环境</h3></div></div></div>
			
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1190"></a>20.1.1.1. Mac</h4></div></div></div>
				
				<pre class="screen">
neo@MacBook-Pro ~ % brew install python3
neo@MacBook-Pro ~ % pip3 install scrapy
			</pre>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1191"></a>20.1.1.2. Ubuntu</h4></div></div></div>
				
				<p>搜索 scrapy 包，scrapy 支持 Python2.7 和 Python3 我们只需要 python3 版本
				</p>
				<pre class="screen">
neo@netkiller ~ % apt-cache search scrapy | grep python3
python3-scrapy - Python web scraping and crawling framework (Python 3)
python3-scrapy-djangoitem - Scrapy extension to write scraped items using Django models (Python3 version)
python3-w3lib - Collection of web-related functions (Python 3)			
			</pre>
				<p>Ubuntu 17.04 默认 scrapy 版本为 1.3.0-1 如果需要最新的 1.4.0 请使用 pip 命令安装
				</p>
				<pre class="screen">
neo@netkiller ~ % apt search python3-scrapy
Sorting... Done
Full Text Search... Done
python3-scrapy/zesty,zesty 1.3.0-1~exp2 all
  Python web scraping and crawling framework (Python 3)

python3-scrapy-djangoitem/zesty,zesty 1.1.1-1 all
  Scrapy extension to write scraped items using Django models (Python3 version)
			</pre>
				<p>安装 scrapy</p>
				<pre class="screen">
neo@netkiller ~ % sudo apt install python3-scrapy
[sudo] password for neo: 
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  ipython3 libmysqlclient20 libwebpmux2 mysql-common python-pexpect python-ptyprocess python3-attr python3-boto python3-bs4 python3-cffi-backend python3-click python3-colorama python3-constantly
  python3-cryptography python3-cssselect python3-decorator python3-html5lib python3-idna python3-incremental python3-ipython python3-ipython-genutils python3-libxml2 python3-lxml python3-mysqldb
  python3-openssl python3-pam python3-parsel python3-pexpect python3-pickleshare python3-pil python3-prompt-toolkit python3-ptyprocess python3-pyasn1 python3-pyasn1-modules python3-pydispatch
  python3-pygments python3-queuelib python3-serial python3-service-identity python3-setuptools python3-simplegeneric python3-traitlets python3-twisted python3-twisted-bin python3-w3lib python3-wcwidth
  python3-webencodings python3-zope.interface
Suggested packages:
  python-pexpect-doc python-attr-doc python-cryptography-doc python3-cryptography-vectors python3-genshi python3-lxml-dbg python-lxml-doc default-mysql-server | virtual-mysql-server
  python-egenix-mxdatetime python3-mysqldb-dbg python-openssl-doc python3-openssl-dbg python3-pam-dbg python-pil-doc python3-pil-dbg doc-base python-pydispatch-doc ttf-bitstream-vera python-scrapy-doc
  python3-wxgtk3.0 | python3-wxgtk python-setuptools-doc python3-tk python3-gtk2 python3-glade2 python3-qt4 python3-wxgtk2.8 python3-twisted-bin-dbg
The following NEW packages will be installed:
  ipython3 libmysqlclient20 libwebpmux2 mysql-common python-pexpect python-ptyprocess python3-attr python3-boto python3-bs4 python3-cffi-backend python3-click python3-colorama python3-constantly
  python3-cryptography python3-cssselect python3-decorator python3-html5lib python3-idna python3-incremental python3-ipython python3-ipython-genutils python3-libxml2 python3-lxml python3-mysqldb
  python3-openssl python3-pam python3-parsel python3-pexpect python3-pickleshare python3-pil python3-prompt-toolkit python3-ptyprocess python3-pyasn1 python3-pyasn1-modules python3-pydispatch
  python3-pygments python3-queuelib python3-scrapy python3-serial python3-service-identity python3-setuptools python3-simplegeneric python3-traitlets python3-twisted python3-twisted-bin python3-w3lib
  python3-wcwidth python3-webencodings python3-zope.interface
0 upgraded, 49 newly installed, 0 to remove and 0 not upgraded.
Need to get 7,152 kB of archives.
After this operation, 40.8 MB of additional disk space will be used.
Do you want to continue? [Y/n]
			</pre>
				<p>输入大写 “Y” 然后回车</p>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1192"></a>20.1.1.3. 使用 pip 安装 scrapy</h4></div></div></div>
				
				<pre class="screen">
neo@netkiller ~ % sudo apt install python3-pip
neo@netkiller ~ % pip3 install scrapy
			</pre>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1193"></a>20.1.1.4. 测试 scrapy</h4></div></div></div>
				
				<p>创建测试程序，用于验证 scrapy 安装是否存在问题。</p>
				<pre class="screen">
			
$ cat &gt; myspider.py &lt;&lt;EOF
import scrapy

class BlogSpider(scrapy.Spider):
    name = 'blogspider'
    start_urls = ['https://blog.scrapinghub.com']

    def parse(self, response):
        for title in response.css('h2.entry-title'):
            yield {'title': title.css('a ::text').extract_first()}

        for next_page in response.css('div.prev-post &gt; a'):
            yield response.follow(next_page, self.parse)
EOF
			
				</pre>
				<p>运行爬虫</p>
				<pre class="screen">
$ scrapy runspider myspider.py
			</pre>
			</div>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="scrapy"></a>20.1.2. scrapy 命令</h3></div></div></div>
			
			<pre class="screen">
		
neo@MacBook-Pro ~/Documents/crawler % scrapy     
Scrapy 1.4.0 - project: crawler

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  bench         Run quick benchmark test
  check         Check spider contracts
  crawl         Run a spider
  edit          Edit spider
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  list          List available spiders
  parse         Parse URL (using its spider) and print the results
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

Use "scrapy &lt;command&gt; -h" to see more info about a command
		
			</pre>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="scrapy.startproject"></a>20.1.2.1. </h4></div></div></div>
				
				<pre class="screen">
			
neo@MacBook-Pro ~/Documents % scrapy startproject crawler 
New Scrapy project 'crawler', using template directory '/usr/local/lib/python3.6/site-packages/scrapy/templates/project', created in:
    /Users/neo/Documents/crawler

You can start your first spider with:
    cd crawler
    scrapy genspider example example.com
			
				</pre>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="scrapy.genspider"></a>20.1.2.2. 新建 spider</h4></div></div></div>
				
				<pre class="screen">
			
neo@MacBook-Pro ~/Documents/crawler % scrapy genspider netkiller netkiller.cn
Created spider 'netkiller' using template 'basic' in module:
  crawler.spiders.netkiller
			
				</pre>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="scrapy.list"></a>20.1.2.3. 列出可用的 spiders</h4></div></div></div>
				
				<pre class="screen">
			
neo@MacBook-Pro ~/Documents/crawler % scrapy list
bing
book
example
netkiller			
			
				</pre>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="scrapy.crawl"></a>20.1.2.4. 运行 spider</h4></div></div></div>
				
				<pre class="screen">
			
neo@MacBook-Pro ~/Documents/crawler % scrapy crawl netkiller
			
				</pre>
				<p>运行结果输出到 json 文件中</p>
				<pre class="screen">
			
neo@MacBook-Pro ~/Documents/crawler % scrapy crawl netkiller -o output.json					
			
				</pre>
			</div>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="shell"></a>20.1.3. Scrapy Shell</h3></div></div></div>
			
			<p>Scrapy Shell 是一个爬虫命令行交互界面调试工具，可以使用它分析被爬的页面</p>
			<pre class="screen">
		
neo@MacBook-Pro /tmp % scrapy shell http://www.netkiller.cn
2017-09-01 15:23:05 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)
2017-09-01 15:23:05 [scrapy.utils.log] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}
2017-09-01 15:23:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2017-09-01 15:23:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-01 15:23:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-01 15:23:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-09-01 15:23:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-09-01 15:23:05 [scrapy.core.engine] INFO: Spider opened
2017-09-01 15:23:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.netkiller.cn&gt; (referer: None)
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x103b2afd0&gt;
[s]   item       {}
[s]   request    &lt;GET http://www.netkiller.cn&gt;
[s]   response   &lt;200 http://www.netkiller.cn&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x1049019e8&gt;
[s]   spider     &lt;DefaultSpider 'default' at 0x104be2a90&gt;
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects 
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
&gt;&gt;&gt; 
		
			</pre>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="response"></a>20.1.3.1. response</h4></div></div></div>
				
				<p>response 是爬虫返回的页面，可以通过 css(), xpath() 等方法取出你需要的内容。</p>

				<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="response.url"></a>当前URL地址</h5></div></div></div>
					
					<pre class="screen">
				
&gt;&gt;&gt; response.url
'https://netkiller.cn/linux/index.html'
				
					</pre>
				</div>
				<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="status"></a>status HTTP 状态</h5></div></div></div>
					
					<pre class="screen">
				
&gt;&gt;&gt; response.status
200
				
					</pre>
				</div>
				<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="text"></a>text 正文</h5></div></div></div>
					
					<p>返回 HTML 页面正文</p>
					<pre class="screen">
response.text
				</pre>
				</div>
				<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="response.css"></a>css</h5></div></div></div>
					
					<p>css() 这个方法可以用来选择html和css</p>
					<pre class="screen">
				
&gt;&gt;&gt; response.css('title')
[&lt;Selector xpath='descendant-or-self::title' data='&lt;title&gt;Netkiller ebook - Linux ebook&lt;/ti'&gt;]

&gt;&gt;&gt; response.css('title').extract()
['&lt;title&gt;Netkiller ebook - Linux ebook&lt;/title&gt;']

&gt;&gt;&gt; response.css('title::text').extract()
['Netkiller ebook - Linux ebook']
				
					</pre>
					<p>基于 class 选择</p>
					<pre class="screen">
				
&gt;&gt;&gt; response.css('a.ulink')[1].extract()
'&lt;a class="ulink" href="http://netkiller.github.io/" target="_top"&gt;http://netkiller.github.io&lt;/a&gt;'	

&gt;&gt;&gt; response.css('a.ulink::text')[3].extract()
'http://netkiller.sourceforge.net'
				
					</pre>
					<p>数组的处理</p>
					<pre class="screen">
				
&gt;&gt;&gt; response.css('a::text').extract_first()
'简体中文'

&gt;&gt;&gt; response.css('a::text')[1].extract()
'繁体中文'

&gt;&gt;&gt; response.css('div.blockquote')[1].css('a.ulink::text').extract()
['Netkiller Architect 手札', 'Netkiller Developer 手札', 'Netkiller PHP 手札', 'Netkiller Python 手札', 'Netkiller Testing 手札', 'Netkiller Java 手札', 'Netkiller Cryptography 手札', 'Netkiller Linux 手札', 'Netkiller FreeBSD 手札', 'Netkiller Shell 手札', 'Netkiller Security 手札', 'Netkiller Web 手札', 'Netkiller Monitoring 手札', 'Netkiller Storage 手札', 'Netkiller Mail 手札', 'Netkiller Docbook 手札', 'Netkiller Project 手札', 'Netkiller Database 手札', 'Netkiller PostgreSQL 手札', 'Netkiller MySQL 手札', 'Netkiller NoSQL 手札', 'Netkiller LDAP 手札', 'Netkiller Network 手札', 'Netkiller Cisco IOS 手札', 'Netkiller H3C 手札', 'Netkiller Multimedia 手札', 'Netkiller Perl 手札', 'Netkiller Amateur Radio 手札']
				
					</pre>
					<p>正则表达式</p>
					<pre class="screen">
				
&gt;&gt;&gt; response.css('title::text').re(r'Netkiller.*')
['Netkiller ebook - Linux ebook']

&gt;&gt;&gt; response.css('title::text').re(r'N\w+')
['Netkiller']

&gt;&gt;&gt; response.css('title::text').re(r'(\w+) (\w+)')
['Netkiller', 'ebook', 'Linux', 'ebook']
				
					</pre>
					<div class="section"><div class="titlepage"><div><div><h6 class="title"><a id="id1194"></a>获取 html 属性</h6></div></div></div>
						
						<p>通过 a::attr() 可以获取 html 标记的属性值</p>
						<pre class="screen">
					
&gt;&gt;&gt; response.css('td a::attr(href)').extract_first()
'http://netkiller.github.io/'					
					
						</pre>
					</div>
				</div>
				<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="response.xpath"></a>xpath</h5></div></div></div>
					
					<pre class="screen">
				
&gt;&gt;&gt; response.xpath('//title')
[&lt;Selector xpath='//title' data='&lt;title&gt;Netkiller ebook - Linux ebook&lt;/ti'&gt;]

&gt;&gt;&gt; response.xpath('//title/text()').extract_first()
'Netkiller ebook - Linux ebook'				
				
					</pre>
					<p>xpath 也可以使用 re() 方法做正则处理</p>
					<pre class="screen">
				
&gt;&gt;&gt; response.xpath('//title/text()').re(r'(\w+)')
['Netkiller', 'ebook', 'Linux', 'ebook']	

&gt;&gt;&gt; response.xpath('//div[@class="time"]/text()').re('[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}')
['2017-09-21 02:01:38']	
				
					</pre>
					<p>抽取HTML属性值，如图片URL。</p>
					<pre class="screen">
				
&gt;&gt;&gt; response.xpath('//img/@src').extract()
['graphics/spacer.gif', 'graphics/note.gif', 'graphics/by-nc-sa.png', '/images/weixin.jpg', 'images/neo.jpg', '/images/weixin.jpg']
				
					</pre>
					<p>筛选 class</p>
					<pre class="screen">
				
&gt;&gt;&gt; response.xpath('//a/@href')[0].extract()
'http://netkiller.github.io/'

&gt;&gt;&gt; response.xpath('//a/text()')[0].extract()
'简体中文'

&gt;&gt;&gt; response.xpath('//div[@class="blockquote"]')[1].css('a.ulink::text').extract()
['Netkiller Architect 手札', 'Netkiller Developer 手札', 'Netkiller PHP 手札', 'Netkiller Python 手札', 'Netkiller Testing 手札', 'Netkiller Java 手札', 'Netkiller Cryptography 手札', 'Netkiller Linux 手札', 'Netkiller FreeBSD 手札', 'Netkiller Shell 手札', 'Netkiller Security 手札', 'Netkiller Web 手札', 'Netkiller Monitoring 手札', 'Netkiller Storage 手札', 'Netkiller Mail 手札', 'Netkiller Docbook 手札', 'Netkiller Project 手札', 'Netkiller Database 手札', 'Netkiller PostgreSQL 手札', 'Netkiller MySQL 手札', 'Netkiller NoSQL 手札', 'Netkiller LDAP 手札', 'Netkiller Network 手札', 'Netkiller Cisco IOS 手札', 'Netkiller H3C 手札', 'Netkiller Multimedia 手札', 'Netkiller Perl 手札', 'Netkiller Amateur Radio 手札']

				
					</pre>
					<p>使用 | 匹配多组规则</p>
					<pre class="screen">
				
&gt;&gt;&gt; response.xpath('//ul[@class="topnews_nlist"]/li/h2/a/@href|//ul[@class="topnews_nlist"]/li/a/@href').extract()
				
					</pre>

				</div>
				<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="headers"></a>headers</h5></div></div></div>
					
					<pre class="screen">
response.headers.getlist('Set-Cookie')
				</pre>
				</div>

			</div>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="tutorial"></a>20.1.4. 爬虫项目</h3></div></div></div>
			
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="startproject"></a>20.1.4.1. 创建项目</h4></div></div></div>
				
				<p>创建爬虫项目</p>
				<pre class="screen">
scrapy startproject project
			</pre>
				<p>在抓取之前，你需要新建一个Scrapy工程</p>
				<pre class="screen">
			
neo@MacBook-Pro ~/Documents % scrapy startproject crawler 
New Scrapy project 'crawler', using template directory '/usr/local/lib/python3.6/site-packages/scrapy/templates/project', created in:
    /Users/neo/Documents/crawler

You can start your first spider with:
    cd crawler
    scrapy genspider example example.com

neo@MacBook-Pro ~/Documents % cd crawler 
neo@MacBook-Pro ~/Documents/crawler % find .
.
./crawler
./crawler/__init__.py
./crawler/__pycache__
./crawler/items.py
./crawler/middlewares.py
./crawler/pipelines.py
./crawler/settings.py
./crawler/spiders
./crawler/spiders/__init__.py
./crawler/spiders/__pycache__
./scrapy.cfg

			
				</pre>
				<p>Scrapy 工程目录主要有以下文件组成：</p>
				<pre class="screen">
scrapy.cfg: 项目配置文件
middlewares.py : 项目 middlewares 文件
items.py: 项目items文件
pipelines.py: 项目管道文件
settings.py: 项目配置文件
spiders: 放置spider的目录
			</pre>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="spider"></a>20.1.4.2. Spider</h4></div></div></div>
				
				<p>创建爬虫，名字是 netkiller, 爬行的地址是 netkiller.cn</p>
				<pre class="screen">
			
neo@MacBook-Pro ~/Documents/crawler % scrapy genspider netkiller netkiller.cn
Created spider 'netkiller' using template 'basic' in module:
  crawler.spiders.netkiller
neo@MacBook-Pro ~/Documents/crawler % find .
.
./crawler
./crawler/__init__.py
./crawler/__pycache__
./crawler/__pycache__/__init__.cpython-36.pyc
./crawler/__pycache__/settings.cpython-36.pyc
./crawler/items.py
./crawler/middlewares.py
./crawler/pipelines.py
./crawler/settings.py
./crawler/spiders
./crawler/spiders/__init__.py
./crawler/spiders/__pycache__
./crawler/spiders/__pycache__/__init__.cpython-36.pyc
./crawler/spiders/netkiller.py
./scrapy.cfg

			
				</pre>
				<p>打开 crawler/spiders/netkiller.py 文件，修改内容如下</p>
				<pre class="screen">
			
# -*- coding: utf-8 -*-
import scrapy


class NetkillerSpider(scrapy.Spider):
    name = 'netkiller'
    allowed_domains = ['netkiller.cn']
    start_urls = ['http://www.netkiller.cn/']

    def parse(self, response):
        for link in response.xpath('//div[@class="blockquote"]')[1].css('a.ulink'):
            # self.log('This url is %s' % link)
            yield {
                'name': link.css('a::text').extract(),
                'url': link.css('a.ulink::attr(href)').extract()
                }
            
        pass
			
				</pre>
				<p>运行爬虫</p>
				<pre class="screen">
			
neo@MacBook-Pro ~/Documents/crawler % scrapy crawl netkiller -o output.json
2017-09-08 11:42:30 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: crawler)
2017-09-08 11:42:30 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'crawler', 'FEED_FORMAT': 'json', 'FEED_URI': 'output.json', 'NEWSPIDER_MODULE': 'crawler.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['crawler.spiders']}
2017-09-08 11:42:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2017-09-08 11:42:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-08 11:42:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-08 11:42:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-09-08 11:42:30 [scrapy.core.engine] INFO: Spider opened
2017-09-08 11:42:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-09-08 11:42:30 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-09-08 11:42:30 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.netkiller.cn/robots.txt&gt; (referer: None)
2017-09-08 11:42:31 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.netkiller.cn/&gt; (referer: None)
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Architect 手札'], 'url': ['../architect/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Developer 手札'], 'url': ['../developer/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller PHP 手札'], 'url': ['../php/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Python 手札'], 'url': ['../python/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Testing 手札'], 'url': ['../testing/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Java 手札'], 'url': ['../java/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Cryptography 手札'], 'url': ['../cryptography/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Linux 手札'], 'url': ['../linux/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller FreeBSD 手札'], 'url': ['../freebsd/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Shell 手札'], 'url': ['../shell/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Security 手札'], 'url': ['../security/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Web 手札'], 'url': ['../www/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Monitoring 手札'], 'url': ['../monitoring/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Storage 手札'], 'url': ['../storage/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Mail 手札'], 'url': ['../mail/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Docbook 手札'], 'url': ['../docbook/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Project 手札'], 'url': ['../project/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Database 手札'], 'url': ['../database/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller PostgreSQL 手札'], 'url': ['../postgresql/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller MySQL 手札'], 'url': ['../mysql/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller NoSQL 手札'], 'url': ['../nosql/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller LDAP 手札'], 'url': ['../ldap/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Network 手札'], 'url': ['../network/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Cisco IOS 手札'], 'url': ['../cisco/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller H3C 手札'], 'url': ['../h3c/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Multimedia 手札'], 'url': ['../multimedia/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Perl 手札'], 'url': ['../perl/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Amateur Radio 手札'], 'url': ['../radio/index.html']}
2017-09-08 11:42:31 [scrapy.core.engine] INFO: Closing spider (finished)
2017-09-08 11:42:31 [scrapy.extensions.feedexport] INFO: Stored json feed (28 items) in: output.json
2017-09-08 11:42:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 438,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 6075,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 9, 8, 3, 42, 31, 157395),
 'item_scraped_count': 28,
 'log_count/DEBUG': 31,
 'log_count/INFO': 8,
 'memusage/max': 49434624,
 'memusage/startup': 49434624,
 'response_received_count': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 9, 8, 3, 42, 30, 931267)}
2017-09-08 11:42:31 [scrapy.core.engine] INFO: Spider closed (finished)
			
				</pre>
				<p>你会看到返回结果</p>
				<pre class="screen">
			
{'name': ['Netkiller Architect 手札'], 'url': ['../architect/index.html']}			
			
				</pre>
				<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="response.follow"></a>翻页操作</h5></div></div></div>
					
					<p>下面我们演示爬虫翻页，例如我们需要遍历这部电子书《Netkiller Linux 手札》
						https://netkiller.cn/linux/index.html，首先创建一个爬虫任务</p>
					<pre class="screen">
				
neo@MacBook-Pro ~/Documents/crawler % scrapy genspider book netkiller.cn
Created spider 'book' using template 'basic' in module:
  crawler.spiders.book
				
					</pre>
					<p>编辑爬虫任务</p>
					<pre class="screen">
				
# -*- coding: utf-8 -*-
import scrapy


class BookSpider(scrapy.Spider):
    name = 'book'
    allowed_domains = ['netkiller.cn']
    start_urls = ['https://netkiller.cn/linux/index.html']

    def parse(self, response):
        yield {'title': response.css('title::text').extract()}
        # 这里取出下一页连接地址
        next_page = response.xpath('//a[@accesskey="n"]/@href').extract_first() 
        self.log('Next page: %s' % next_page)
        # 如果页面不为空交给 response.follow 来爬取这个页面
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)    

        pass
				
					</pre>

				</div>
				<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="response.body"></a>采集内容保存到文件</h5></div></div></div>
					
					<p>下面的例子是将 response.body 返回采集内容保存到文件中</p>
					<pre class="screen">
				
# -*- coding: utf-8 -*-
import scrapy


class BookSpider(scrapy.Spider):
    name = 'book'
    allowed_domains = ['netkiller.cn']
    start_urls = ['https://netkiller.cn/linux/index.html']

    def parse(self, response):
        yield {'title': response.css('title::text').extract()}

        filename = '/tmp/%s' % response.url.split("/")[-1]

        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)

        next_page = response.xpath('//a[@accesskey="n"]/@href').extract_first()
        self.log('Next page: %s' % next_page)
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)    

        pass
				
				
					</pre>
					<p>任务运维结束后查看采集出来的文件</p>
					<pre class="screen">
				
neo@MacBook-Pro ~/Documents/crawler % ls /tmp/*.html
/tmp/apt-get.html            /tmp/disc.html               /tmp/infomation.html         /tmp/lspci.html              /tmp/svgatextmode.html
/tmp/aptitude.html           /tmp/dmidecode.html          /tmp/install.html            /tmp/lsscsi.html             /tmp/swap.html
/tmp/author.html             /tmp/do-release-upgrade.html /tmp/install.partition.html  /tmp/lsusb.html              /tmp/sys.html
/tmp/avlinux.html            /tmp/dpkg.html               /tmp/introduction.html       /tmp/package.html            /tmp/sysctl.html
/tmp/centos.html             /tmp/du.max-depth.html       /tmp/kernel.html             /tmp/pr01s02.html            /tmp/system.infomation.html
/tmp/cfdisk.html             /tmp/ethtool.html            /tmp/kernel.modules.html     /tmp/pr01s03.html            /tmp/system.profile.html
/tmp/console.html            /tmp/framebuffer.html        /tmp/kudzu.html              /tmp/pr01s05.html            /tmp/system.shutdown.html
/tmp/console.timeout.html    /tmp/gpt.html                /tmp/linux.html              /tmp/preface.html            /tmp/tune2fs.html
/tmp/dd.clone.html           /tmp/hdd.label.html          /tmp/locale.html             /tmp/proc.html               /tmp/udev.html
/tmp/deb.html                /tmp/hdd.partition.html      /tmp/loop.html               /tmp/rpm.html                /tmp/upgrades.html
/tmp/device.cpu.html         /tmp/hwinfo.html             /tmp/lsblk.html              /tmp/rpmbuild.html           /tmp/yum.html
/tmp/device.hba.html         /tmp/index.html              /tmp/lshw.html               /tmp/smartctl.html				
				
					</pre>
					<p>这里只是做演示，生产环境请不要在 parse(self, response) 中处理，后面会讲到 Pipeline。
					</p>
				</div>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="settings"></a>20.1.4.3. settings.py 爬虫配置文件</h4></div></div></div>
				
				<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id1195"></a>忽略 robots.txt 规则</h5></div></div></div>
					
					<pre class="screen">
				
# Obey robots.txt rules
ROBOTSTXT_OBEY = False
				
					</pre>
				</div>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="items"></a>20.1.4.4. Item</h4></div></div></div>
				
				<p>Item 在 scrapy
					中的类似“实体”或者“POJO”的概念，是一个数据结构类。爬虫通过ItemLoader将数据放到Item中</p>
				<p>下面是 items.py 文件</p>
				<pre class="screen">
			
# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/items.html

import scrapy


class CrawlerItem(scrapy.Item):
    # define the fields for your item here like:
    title = scrapy.Field()
    author = scrapy.Field()
    content = scrapy.Field()
    ctime = scrapy.Field()
    
    pass

			
				</pre>
				<p>下面是爬虫文件</p>
				<pre class="screen">
			
# -*- coding: utf-8 -*-
import scrapy
from scrapy.loader import ItemLoader
from crawler.items import CrawlerItem 
import time

class ExampleSpider(scrapy.Spider):
    name = 'example'
    allowed_domains = ['netkiller.cn']
    start_urls = ['https://netkiller.cn/java/index.html']
    def parse(self, response):

        item_selector = response.xpath('//a/@href')
        for url in item_selector.extract():
            if 'html' in url.split('.'):
                url = response.urljoin(url)
                yield response.follow( url, callback=self.parse_item)

        next_page = response.xpath('//a[@accesskey="n"]/@href').extract_first()
        self.log('Next page: %s' % next_page)
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)   
        
    def parse_item(self, response):
        l = ItemLoader(item=CrawlerItem(), response=response)
        l.add_css('title', 'title::text')
        l.add_value('ctime', time.strftime( '%Y-%m-%d %X', time.localtime() ))
        l.add_value('content', response.body)
        return l.load_item()
			
				</pre>
				<p>yield response.follow( url, callback=self.parse_item) 会回调
					parse_item(self, response) 将爬到的数据放置到 Item 中</p>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="pipeline"></a>20.1.4.5. Pipeline</h4></div></div></div>
				
				<p>Pipeline 管道线，主要的功能是对 Item
					的数据处理，例如计算、合并等等。通常我们在这里做数据保存。下面的例子是将爬到的数据保存到 json 文件中。</p>
				<p>默认情况 Pipeline 是禁用的，首先我们需要开启 Pipeline 支持，修改 settings.py
					文件，找到下面配置项，去掉注释。</p>
				<pre class="screen">
			
# Configure item pipelines
# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
    'crawler.pipelines.CrawlerPipeline': 300,
}
			
				</pre>
				<p>修改 pipelines.py 文件。</p>
				<pre class="screen">
			
# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html

import json

class CrawlerPipeline(object):
    def open_spider(self, spider):
        self.file = open('items.json', 'w')

    def close_spider(self, spider):
        self.file.close()
    def process_item(self, item, spider):
        # self.log("PIPE: %s" % item)
        line = json.dumps(dict(item)) + "\n"
        self.file.write(line)   
        return item

			
				</pre>
				<p>下面是 items.py 文件</p>
				<pre class="screen">
			
# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/items.html

import scrapy


class CrawlerItem(scrapy.Item):
    # define the fields for your item here like:
    title = scrapy.Field()
    author = scrapy.Field()
    content = scrapy.Field()
    ctime = scrapy.Field()
    
    pass

			
				</pre>
				<p>下面是爬虫文件</p>
				<pre class="screen">
			
# -*- coding: utf-8 -*-
import scrapy
from scrapy.loader import ItemLoader
from crawler.items import CrawlerItem 
import time

class ExampleSpider(scrapy.Spider):
    name = 'example'
    allowed_domains = ['netkiller.cn']
    start_urls = ['https://netkiller.cn/java/index.html']
    def parse(self, response):

        item_selector = response.xpath('//a/@href')
        for url in item_selector.extract():
            if 'html' in url.split('.'):
                url = response.urljoin(url)
                yield response.follow( url, callback=self.parse_item)

        next_page = response.xpath('//a[@accesskey="n"]/@href').extract_first()
        self.log('Next page: %s' % next_page)
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)   
        
    def parse_item(self, response):
        l = ItemLoader(item=CrawlerItem(), response=response)
        l.add_css('title', 'title::text')
        l.add_value('ctime', time.strftime( '%Y-%m-%d %X', time.localtime() ))
        l.add_value('content', response.body)
        return l.load_item()
			
				</pre>
				<p>items.json 文件如下</p>
				<pre class="screen">
			
{"title": ["5.31.\u00a0Spring boot with Data restful"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.30.\u00a0Spring boot with Phoenix"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.29.\u00a0Spring boot with Apache Hive"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.28.\u00a0Spring boot with Elasticsearch 5.5.x"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.27.\u00a0Spring boot with Elasticsearch 2.x"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.23.\u00a0Spring boot with Hessian"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.22.\u00a0Spring boot with Cache"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.26.\u00a0Spring boot with HTTPS SSL"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.25.\u00a0Spring boot with Git version"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.24.\u00a0Spring boot with Apache Kafka"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.21.\u00a0Spring boot with Scheduling"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.20.\u00a0Spring boot with Oauth2"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.19.\u00a0Spring boot with Spring security"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.16.\u00a0Spring boot with PostgreSQL"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.18.\u00a0Spring boot with Velocity template"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.13.\u00a0Spring boot with MongoDB"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.11.\u00a0Spring boot with Session share"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.17.\u00a0Spring boot with Email"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.15.\u00a0Spring boot with Oracle"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.14.\u00a0Spring boot with MySQL"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.10.\u00a0Spring boot with Logging"], "ctime": ["2017-09-11 11:57:53"]}
{"title": ["5.9.\u00a0String boot with RestTemplate"], "ctime": ["2017-09-11 11:57:53"]}			
			
				</pre>
			</div>


		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="images"></a>20.1.5. 下载图片</h3></div></div></div>
			
			<p>创建项目</p>
			<pre class="screen">
		
neo@MacBook-Pro ~/Documents % scrapy startproject photo			
		
			</pre>
			<pre class="screen">
		
neo@MacBook-Pro ~/Documents % cd photo
		
			</pre>
			<p>安装依赖库</p>
			<pre class="screen">
		
neo@MacBook-Pro ~/Documents/photo % pip3 install image		
		
			</pre>
			<p>创建爬虫</p>
			<pre class="screen">
		
neo@MacBook-Pro ~/Documents/photo % scrapy genspider jiandan jandan.net			
		
			</pre>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1196"></a>20.1.5.1. 配置 settings.py</h4></div></div></div>
				
				<p>忽略 robots.txt 规则</p>
				<pre class="screen">
			
# Obey robots.txt rules
ROBOTSTXT_OBEY = False
			
				</pre>
				<p>配置图片保存路径与缩图</p>
				<pre class="screen">
			
#图片保存路径
IMAGES_STORE='/tmp/photo'
#DOWNLOAD_DELAY = 0.25
#缩略图的尺寸，设置这个值就会产生缩略图
IMAGES_THUMBS = {
    'small': (50, 50),
    'big': (200, 200),
}				
			
				</pre>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1197"></a>20.1.5.2. 修改 pipelines.py 文件</h4></div></div></div>
				
				<p>加入 process_item（）与 item_completed（） 方法</p>
				<p>注意：PhotoPipeline(ImagesPipeline) 需要继承 ImagesPipeline</p>
				<pre class="screen">
			
# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html

import scrapy
from scrapy.pipelines.images import ImagesPipeline
from scrapy.exceptions import DropItem

class PhotoPipeline(ImagesPipeline):
    # def process_item(self, item, spider):
        # return item
    def get_media_requests(self, item, info):
        for image_url in item['image_urls']:
            yield scrapy.http.Request('http:'+image_url)

    def item_completed(self, results, item, info):
        image_paths = [x['path'] for ok, x in results if ok]
        if not image_paths:
            raise DropItem("Item contains no images")
        item['image_paths'] = image_paths
        return item
			
				</pre>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1198"></a>20.1.5.3. 编辑 items.py</h4></div></div></div>
				
				<p>忽略 robots.txt 规则</p>
				<pre class="screen">
			
# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/items.html

import scrapy


class PhotoItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    #图片的链接
    image_urls = scrapy.Field()
    images = scrapy.Field()
    image_paths = scrapy.Field()
    pass
			
				</pre>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1199"></a>20.1.5.4. Spider 爬虫文件</h4></div></div></div>
				
				<pre class="screen">
			
# -*- coding: utf-8 -*-
import scrapy
from scrapy.loader import ItemLoader
from photo.items import PhotoItem

class JiandanSpider(scrapy.Spider):
    name = 'jiandan'
    # allowed_domains = ['jandan.net']
    allowed_domains = [] 
    start_urls = ['http://jandan.net/ooxx']

    def parse(self, response):
       
        l = ItemLoader(item=PhotoItem(), response=response)
        l.add_xpath('image_urls','//img//@src' )
        yield l.load_item()

        next_page = response.xpath('//a[@class="previous-comment-page"]//@href').extract_first() #翻页
        if next_page:
            yield response.follow(next_page,self.parse)
        pass
    def parse_page(self, response):
        l = ItemLoader(item=PhotoItem(), response=response)
        l.add_xpath('image_urls','//img//@src' )
        return l.load_item()				
			
				</pre>
			</div>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="xpath"></a>20.1.6. xpath</h3></div></div></div>
			
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id1202"></a>20.1.6.1. 逻辑运算符</h4></div></div></div>
				
				<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id1200"></a>and</h5></div></div></div>
					
					<pre class="screen">
				
&gt;&gt;&gt; response.xpath('//span[@class="time" and @id="news-time"]/text()').extract()
['2017-10-09 09:46']				
				
					</pre>
				</div>
				<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="id1201"></a>or</h5></div></div></div>
					
					<pre class="screen">
				
//*[@class='foo' or contains(@class,' foo ') or starts-with(@class,'foo ') or substring(@class,string-length(@class)-3)=' foo']				
				
					</pre>
				</div>
			</div>
			<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="fun"></a>20.1.6.2. function</h4></div></div></div>
				
				<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="text()"></a>text()</h5></div></div></div>
					
					<pre class="screen">
				
&gt;&gt;&gt; response.xpath('//title/text()').extract_first()
'Netkiller ebook - Linux ebook'				
				
					</pre>
				</div>
				<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="contains()"></a>contains()</h5></div></div></div>
					
					<p>contains() 匹配含有特定字符串的 class</p>
					<pre class="screen">
				
//*[contains(@class,'foo')]
				
					</pre>
					<pre class="screen">
				
&gt;&gt;&gt; response.xpath('//ul[contains(@class, "topnews_nlist")]/li/h2/a/@href|//ul[contains(@class, "topnews_nlist")]/li/a/@href').extract()	
				
					</pre>
					<p>内容匹配</p>
					<pre class="screen">
				
&gt;&gt;&gt; response.xpath('//div[@id="epContentLeft"]/h1[contains(text(),"10")]/text()').extract()
['美联储10月起启动渐进式缩表 维持基准利率不变']					
				
					</pre>
				</div>
			</div>
		</div>
	</div>
</div><script xmlns="" type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=r5HG&amp;d=9mi5r_kkDC8uxG8HuY3p4-2qgeeVypAK9vMD-2P6BYM"></script><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="../pt04.html">上一页</a> </td><td width="20%" align="center"><a accesskey="u" href="../pt04.html">上一级</a></td><td width="40%" align="right"> <a accesskey="n" href="../pandas/index.html">下一页</a></td></tr><tr><td width="40%" align="left" valign="top">部分 IV. Python 数据分析 </td><td width="20%" align="center"><a accesskey="h" href="../index.html">起始页</a></td><td width="40%" align="right" valign="top"> 第 21 章 Pandas - Python Data Analysis Library</td></tr></table></div><script xmlns="">
			(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

			ga('create', 'UA-11694057-1', 'auto');
			ga('send', 'pageview');

		</script><script xmlns="" async="async">
			var _hmt = _hmt || [];
			(function() {
			var hm = document.createElement("script");
			hm.src = "https://hm.baidu.com/hm.js?93967759a51cda79e49bf4e34d0b0f2c";
			var s = document.getElementsByTagName("script")[0];
			s.parentNode.insertBefore(hm, s);
			})();
</script><script xmlns="" async="async">
			(function(){
			var bp = document.createElement('script');
			var curProtocol = window.location.protocol.split(':')[0];
			if (curProtocol === 'https') {
			bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
			}
			else {
			bp.src = 'http://push.zhanzhang.baidu.com/push.js';
			}
			var s = document.getElementsByTagName("script")[0];
			s.parentNode.insertBefore(bp, s);
			})();
</script></body></html>